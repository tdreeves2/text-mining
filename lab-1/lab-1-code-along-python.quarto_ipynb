{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Text Mining Basics with Python!\"\n",
        "subtitle: \"Lab 1: Code-Along\"\n",
        "\n",
        "format:\n",
        "  revealjs: \n",
        "    slide-number: c/t\n",
        "    progress: true\n",
        "    chalkboard: \n",
        "      buttons: false\n",
        "    preview-links: auto\n",
        "    logo: img/LASERLogoB.png\n",
        "    theme: [default, css/laser.scss]\n",
        "    width: 1920\n",
        "    height: 1080\n",
        "    margin: 0.05\n",
        "    footer: <a href=https://www.go.ncsu.edu/laser-institute>go.ncsu.edu/laser-institute\n",
        "#resources:\n",
        "#  - demo.pdf\n",
        "#bibliography: lit/references.bib\n",
        "editor: visual\n",
        "---\n",
        "\n",
        "\n",
        "## Agenda\n",
        "\n",
        "::: columns\n",
        "::: {.column width=\"40%\"}\n",
        "1.  Setting Up Python Environment\n",
        "\n",
        "    -   Install and Import Library\n",
        "\n",
        "2.  Importing and Subsetting Data\n",
        "\n",
        "    -   Read Data\n",
        "    -   Subset Columns\n",
        "    -   Rename Columns\n",
        "    -   Subset Rows\n",
        "\n",
        "3.  Preprocessing Data\n",
        "\n",
        "    -   Tokenize\n",
        "    -   Remove Stop Words\n",
        "    -   Lemmatize\n",
        ":::\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "4.  Exploring Data\n",
        "\n",
        "    -   Word Counts\n",
        "    -   Word Frequencies\n",
        "    -   Term Frequency-Inverse Document Frequency\n",
        "\n",
        "5.  Visualizing Data\n",
        "\n",
        "    -   Word Clouds\n",
        "    -   Bar Chart\n",
        "    -   Small Multiples\n",
        ":::\n",
        ":::\n",
        "\n",
        "# Setting Up Python Environment\n",
        "\n",
        "## Install and Import Library\n",
        "\n",
        "### What is a library?\n",
        "\n",
        "A library is a collection of pre-written code that adds functionality to Python, ensuring common functionalities do not need to be rewritten from scratch.\n",
        "\n",
        "## How to install and import library?\n"
      ],
      "id": "661a1d84"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "!pip install pandas\n",
        "import pandas as pd"
      ],
      "id": "3c26849e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`pip` is Python's library installer and manager.\n",
        "\n",
        "-   `pip install <library_name>`: Install libraries using this command in the terminal.\n",
        "-   `!pip install <library_name>`: In Quarto Markdown (.qmd) files, add an exclamation mark (!) before `pip` to install libraries.\n",
        "-   `import <library_name>`: Load libraries into the Python environment after installation.\n",
        "-   `import <library_name> as <alias>`: Use aliases for commonly used libraries with long names.\n",
        "\n",
        "## Necessary Libraries for Text Mining\n",
        "\n",
        "-   [pandas](https://pandas.pydata.org/): a library for data manipulation and analysis. \n",
        "\n",
        "-   [nltk](https://www.nltk.org/): a ibrary for symbolic and statistical natural language processing for English written in the Python programming language. \n",
        "\n",
        "-   [matplotlib](https://matplotlib.org/): a library for creating static, animated, and interactive visualizations in Python.\n",
        "\n",
        "-   [seaborn](https://seaborn.pydata.org/): a data visualization library based on matplotlib that provides a high-level interface for drawing attractive and informative statistical graphics.\n",
        "\n",
        "-   [scikit-learn](https://scikit-learn.org/stable/): a library that implements a range of machine learning, pre-processing, cross-validation, and visualization algorithms using a unified interface. \n",
        "\n",
        "-   [gensim](https://pypi.org/project/gensim/): a library designed for natural language processing (NLP) tasks such as topic modeling, document indexing, and similarity retrieval, particularly with large text corpora.\n",
        "\n",
        "## Your Turn\n",
        "\n",
        "Install and import all the necessary libraries. Notice that `import matplotlib.pyplot` and use `plt` as its alias, and `import seaborn` and use `sns` as its alias.\n"
      ],
      "id": "ba15f6af"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "189813a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "results": "hide"
      },
      "source": [
        "#| echo: false\n",
        "\n",
        "!pip install nltk matplotlib seaborn scikit-learn\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "id": "bcbe108d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Importing and Subsetting Data\n",
        "\n",
        "## Read Data\n",
        "\n",
        "### How to read data into Python environment?\n"
      ],
      "id": "c360a873"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "opd_survey = pd.read_csv('data/opd_survey.csv', low_memory=False)\n",
        "opd_survey"
      ],
      "id": "9b16f376",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   `pd.read_csv(‘<file_path>’)`: you can use the read_csv() function from pandas to read the csv file into your Python environment.\n",
        "\n",
        "-   `=`: you can use the = to assign the data to a DataFrame and give it a name (i.e., opd_survey).\n",
        "\n",
        "### Important Terminology\n",
        "\n",
        "-   DataFrame and Series are two fundamental data structures in pandas.   \n",
        "    -   A Series is a one-dimensional labeled array capable of holding data of any type (integer, string, float, Python objects, etc.).\n",
        "\n",
        "    -   A DataFrame is a two-dimensional, size-mutable, potentially heterogeneous tabular data structure with labeled axes (rows and columns). It is similar in concept to a spreadsheet or SQL table, but with additional powerful capabilities for data manipulation and analysis.\n",
        "\n",
        "## Read Data\n",
        "\n",
        "### How to read other data file types?\n",
        "\n",
        "-   `pd.read_excel('<file_path>')`\n",
        "-   `pd.read_json('<file_path>')`\n",
        "\n",
        "## Your Turn\n",
        "\n",
        "Read the CSV file called `opd_survey_copy` within the data folder and create a DataFrame corresponding to its name.\n"
      ],
      "id": "09efe3ea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "3ed847ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Subset Columns\n",
        "\n",
        "### Why subset columns?\n",
        "\n",
        "-   Focus on relevant data\n",
        "-   Privacy and ethical considerations\n",
        "\n",
        "## Subset Columns\n",
        "\n",
        "### How to subset columns?\n"
      ],
      "id": "dd62c4f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "opd_selected = opd_survey[['Role', 'Resource', 'Q21']]\n",
        "opd_selected"
      ],
      "id": "15202c41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   `df[['<column_name1>', '<column_name2>', ...]]`: Select columns by names.\n",
        "-   `df.iloc[:, [<index1>, <index2>, ...]]`: Select columns by positional index.\n",
        "-   `df.drop(columns=['<column_name1>', '<column_name2>'])`: Drop unwanted columns.\n",
        "\n",
        "## Your Turn\n",
        "\n",
        "Select these three columns `Role`, `Resource`, `Q21` using the index approach.\n"
      ],
      "id": "efeeffe8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "1054df53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rename Columns\n",
        "\n",
        "### Why rename columns?\n",
        "\n",
        "-   Clarity and readability\n",
        "-   Consistency across datasets\n",
        "\n",
        "## Rename Columns\n",
        "\n",
        "### How to rename a column?\n"
      ],
      "id": "a27ac163"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "opd_renamed = opd_selected.rename(columns = {'Q21':'text'})\n",
        "opd_renamed"
      ],
      "id": "19ca0fb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   `df.rename(columns={'<old_name>': '<new_name>'}, inplace=True)`: Rename columns.\n",
        "-   `df.columns = ['<new_name1>', '<new_name2>', ...]`: Directly assign new column names.\n",
        "\n",
        "## Your Turn\n",
        "\n",
        "Rename the `Role` as `role` and `Resource` as `resource`.\n"
      ],
      "id": "93b528c9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "257446a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Subset Rows\n",
        "\n",
        "### Why subset rows?\n",
        "\n",
        "-   Filtering data to meet specific conditions\n",
        "-   Exploratory analysis with small dataset portions\n",
        "-   Preparing data for modeling\n",
        "\n",
        "## Subset Rows\n",
        "\n",
        "### How to subset rows?\n"
      ],
      "id": "d3fb91f9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "opd_sliced = opd_renamed.iloc[2:]\n",
        "opd_teacher = opd_sliced[opd_sliced['Role'] == 'Teacher']\n",
        "opd_teacher"
      ],
      "id": "f9d3d85f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   `df.iloc[[<index1>, <index2>, ...]]`: Select rows based on integer indices.\n",
        "-   `df[df['<column>'] == <condition>]`: Select rows based on conditions.\n",
        "\n",
        "## Your Turn\n",
        "\n",
        "Subset the first 10 rows of `opd_teacher` and save it into a new DataFrame called `opd_teacher_first10`.\n"
      ],
      "id": "762e7813"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "842f68d2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing Data\n",
        "\n",
        "## Tokenize\n",
        "\n",
        "### What is tokenization and why is it needed?\n",
        "\n",
        "Tokenization is the process of breaking down a text into smaller units called tokens, which helps in preparing text for further analysis and modeling by providing a structured representation of linguistic units.\n",
        "\n",
        "### Important Terminology\n",
        "\n",
        "-   **Token**: a single, indivisible unit of text that can be considered as a meaningful component for processing\n",
        "\n",
        "    -   words, punctuation marks, numbers, and other entities that are treated as discrete elements\n",
        "\n",
        "-   **Document**: a unit of text that is being analyzed or processed as a whole\n",
        "\n",
        "    -   a single piece of text, such as an email, a news article, a research paper, etc.\n",
        "\n",
        "    -   a collection of related texts or a segment of a larger corpus\n",
        "\n",
        "-   **Corpus**: a collection of documents\n",
        "\n",
        "## Types of Tokenization\n",
        "\n",
        "-   Word Tokenization\n",
        "\n",
        "    **Example:** \"Hello, world!\" → \\[\"Hello\", \",\", \"world\", \"!\"\\]\n",
        "\n",
        "-   Sentence Tokenization\n",
        "\n",
        "    **Example:** \"Hello world! How are you?\" → \\[\"Hello world!\", \"How are you?\"\\]\n",
        "\n",
        "-   Subword Tokenization\n",
        "\n",
        "    **Example:** \"tokenization\" →\\[\"token\", \"ization\"\\]\n",
        "\n",
        "-   Character Tokenization\n",
        "\n",
        "    **Example:** \"Hello\" → \\[\"H\", \"e\", \"l\", \"l\", \"o\"\\]\n",
        "\n",
        "-   N-gram Tokenization\n",
        "\n",
        "    **Example:** \"Hello world\" with N=2 → \\[\"Hello world\"\\]\n",
        "\n",
        "## How to tokenize?\n",
        "\n",
        "-   Tokenization Module from NLTK\n",
        "\n",
        "    -   work_tokenize\n",
        "\n",
        "    -   RegexpTokenizer\n",
        "\n",
        "    -   WhitespaceTokenizer\n",
        "\n",
        "    -   ...\n",
        "\n",
        "-   Tokenization Module from Spacy \n",
        "\n",
        "-   Tokenization Module from BERT\n",
        "\n",
        "-   ...\n",
        "\n",
        "How to choose the right tokenization method?\n",
        "\n",
        "-   Text Characteristics\n",
        "\n",
        "-   Task Requirements\n",
        "\n",
        "-   Performance Considerations\n",
        "\n",
        "-   ...\n",
        "\n",
        "## RegexpTokenizer\n",
        "\n",
        "RegexpTokenizer uses **regular expressions** to specify the pattern for tokens, allowing you to precisely control what constitutes a token. This flexibility is particularly useful when you need to handle specific tokenization requirements, such as ignoring punctuation, extracting specific patterns like hashtags or mentions in social media texts, or working with languages that don't use whitespace to separate words.\n",
        "\n",
        "## Word Tokenizing a Single Document with RegexpTokenizer\n"
      ],
      "id": "da512533"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "# Import RegexpTokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Define a regex pattern to match words, here the pattern = r'\\w+' means alphanumeric sequences\n",
        "pattern = r'\\w+'\n",
        "\n",
        "# Initiate a RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(pattern)\n",
        "\n",
        "# Example Text\n",
        "text = \"Information is relevant and easily accessable.\"\n",
        "\n",
        "# Tokenize the text \n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "#Display the tokens\n",
        "print(tokens)"
      ],
      "id": "d0a27e1d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word Tokenizing Multiple Documents with Apply & Lambda\n"
      ],
      "id": "71d1db74"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "# Import RegexpTokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Define a regex pattern to match words, here the example pattern = r'\\w+' meaning alphanumeric sequences\n",
        "pattern = r'\\w+'\n",
        "\n",
        "# Initiate a RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(pattern)\n",
        "\n",
        "# Example Corpus\n",
        "corpus = pd.DataFrame({\n",
        "    'document_id': [1, 2, 3],\n",
        "    'text': [\n",
        "        \"Information about how to develop curricula is usefull\",\n",
        "        \"Information of curriculum developing is relevant and accessible.\",\n",
        "        \"Information is of accessibility and affordability.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Tokenize each document in the 'text' column\n",
        "corpus['tokens'] = corpus['text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
        "\n",
        "# Display the DataFrame with tokenized documents\n",
        "print(corpus)"
      ],
      "id": "1f94c584",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply\n"
      ],
      "id": "e8cfd81a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "# Tokenize each document in the 'text' column\n",
        "corpus['tokens'] = corpus['text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
        "\n",
        "# Display the DataFrame with token lists\n",
        "print(corpus)"
      ],
      "id": "0d711a91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### DataFrame.apply(func, axis=0, raw=False, result_type=None, args=(), \\*\\*kwds)\n",
        "\n",
        "-   **func:** The function to apply to each column (axis=0) or row (axis=1).\n",
        "\n",
        "-   **axis:** Axis along which the function is applied (0 for columns, 1 for rows).\n",
        "\n",
        "-   **raw:** Whether to pass the data as ndarray (True) or Series (False).\n",
        "\n",
        "-   **result_type:** Defines whether the result should be 'expand', 'reduce', or 'broadcast'.\n",
        "\n",
        "-   \\*\\*args, **kwds:** Additional arguments and keywords to pass to the function.\n",
        "\n",
        "## Lambda\n"
      ],
      "id": "c03514cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "# Tokenize each document in the 'text' column\n",
        "corpus['tokens'] = corpus['text'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
        "\n",
        "# Display the DataFrame with token lists\n",
        "print(corpus)"
      ],
      "id": "77ed061c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### lambda arguments: expression\n",
        "\n",
        "-   **arguments:** The parameters that the lambda function accepts.\n",
        "-   **expression:** A single expression that is evaluated and returned.\n",
        "\n",
        "## Exploding Tokenized Results\n"
      ],
      "id": "e498cc6d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "column"
      },
      "source": [
        "#| echo: true\n",
        "\n",
        "# Explode the 'word' column to transform each row into individual words\n",
        "corpus_exploded = corpus.explode('tokens')\n",
        "\n",
        "# Display the DataFrame with token into separate rows\n",
        "print(corpus_exploded)"
      ],
      "id": "ae687f73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn\n",
        "\n",
        "Tokenize the `text` column of `opd_teacher` DataFrame and explode the tokenized results into a `word` column.\n"
      ],
      "id": "14e92ef6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "b40af999",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remove Stop Words\n",
        "\n",
        "### Why remove stop words?\n",
        "\n",
        "Stop words like \"and\", \"the\", \"is\", \"in\", \"to\" do not carry important meaning, removing them helps reduce the noise of analysis.\n",
        "\n",
        "## Remove Stop Words\n",
        "\n",
        "### How to remove stop words?\n"
      ],
      "id": "b022dce4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "column"
      },
      "source": [
        "#| echo: true\n",
        "\n",
        "# Import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the NLTK data for stop words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get a list of English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words from the tokens obtained from tokenization\n",
        "corpus_clean = corpus_exploded[~corpus_exploded['tokens'].isin(stop_words)]\n",
        "\n",
        "# Display the DataFrame with exploded tokens and stop words removed\n",
        "print(corpus_clean)"
      ],
      "id": "f8835d81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn\n",
        "\n",
        "Remove stop words from the `word` column in the `opd_teacher` DataFrame.\n"
      ],
      "id": "df04a828"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "30eb9010",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lemmatize\n",
        "\n",
        "### Why lemmatize?\n",
        "\n",
        "Lemmatization is the process of reducing a word to its base or root form, called a lemma. For example, the words \"running\", \"runs\", and \"ran\" are all reduced to their lemma \"run\". Lemmatization makes it easier to compare and analyze texts by ensuring that different forms of a word are treated as a single entity. It helps to improve the accuracy by identifying the true meaning of words and their relationships.\n",
        "\n",
        "## Lemmatize\n",
        "\n",
        "### How to lemmatize?\n"
      ],
      "id": "482eceb7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "column"
      },
      "source": [
        "#| echo: true\n",
        "\n",
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure NLTK resources are downloaded\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the WordNetLemmatizer \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize\n",
        "# Apply lemmatization to each token in the list of tokens\n",
        "corpus_clean['lemmatized_tokens'] = corpus_clean['tokens'].apply(lambda x: lemmatizer.lemmatize(x))\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "print(corpus_clean)"
      ],
      "id": "ac8b1966",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn\n",
        "\n",
        "Lemmatize the `word` column and save it into a new column called `lemmatized_word` in the `opd_teacher` DataFrame.\n"
      ],
      "id": "7206b523"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "4f7ddca7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploring Data\n",
        "\n",
        "## Word Counts\n",
        "\n",
        "### How to count the words?\n",
        "\n",
        "Using `value_counts()`:\n"
      ],
      "id": "32f99198"
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "column"
      },
      "source": [
        "#| echo: true\n",
        "\n",
        "word_counts = corpus_clean['tokens'].value_counts().reset_index()\n",
        "word_counts.columns = ['word', 'count']\n",
        "print(word_counts)"
      ],
      "id": "836a884d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn\n",
        "\n",
        "Count the `word` column in the `opd_teacher` DataFrame.\n"
      ],
      "id": "12595f40"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "5d718782",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word Frequencies\n",
        "\n",
        "### What is word frequency and why is it needed?\n",
        "\n",
        "Word frequencies are the relative counts of words in a unit of texts, usually expressed as a proportion or percentage of the total number of words within the unit of texts. It normalizes the word counts, making them comparable across different texts of varying lengths.\n",
        "\n",
        "## Word Frequencies in the Whole Corpus\n",
        "\n",
        "### How to calculate it?\n",
        "\n",
        "Using `len()`:\n"
      ],
      "id": "2800a404"
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "column"
      },
      "source": [
        "#| echo: true\n",
        "\n",
        "# Calculate the total number of the words in the corpus\n",
        "total_words = len(corpus_clean['tokens'])\n",
        "\n",
        "# Calculate the percentage\n",
        "word_counts['frequency'] = word_counts['count'] / total_words\n",
        "\n",
        "# Display the DataFrame with word frequencies\n",
        "print(word_counts)"
      ],
      "id": "aa5fc7e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word Frequencies in the Documents\n",
        "\n",
        "### How to calculate it?\n",
        "\n",
        "Using `size()` and `sum()` after `groupby()`:\n"
      ],
      "id": "5d2aaa40"
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "column"
      },
      "source": [
        "#| echo: true\n",
        "\n",
        "# Group by the document and count the occurrences of each word\n",
        "word_counts_bydocument = corpus_clean.groupby(['document_id', 'tokens']).size().reset_index(name='count')\n",
        "\n",
        "# Calculate the total number of words in each document\n",
        "total_words = word_counts_bydocument.groupby('document_id')['count'].sum().reset_index(name='total_words')\n",
        "\n",
        "# Merge occurrence of each word and total words of document into one DataFrame, then calculate the percentage\n",
        "word_counts_bydocument = pd.merge(word_counts_bydocument, total_words, on='document_id')\n",
        "\n",
        "# Calculate the frequency of each word\n",
        "word_counts_bydocument['frequency'] = word_counts_bydocument['count'] / word_counts_bydocument['total_words']\n",
        "\n",
        "# Copy the result to a new DataFrame\n",
        "word_frequencies = word_counts_bydocument.copy()\n",
        "\n",
        "# Display the DataFrame with word frequencies\n",
        "print(word_frequencies)"
      ],
      "id": "cb334d7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn\n",
        "\n",
        "Calculate the word frequencies based on `resource` in the `opd_teacher` DataFrame.\n"
      ],
      "id": "c3b0fe2d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "f1364564",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "### What is TF-IDF?\n",
        "\n",
        "TF-IDF is a statistical measure that evaluates how important a word is to a document within a collection or corpus through giving higher weight to words that are frequent in a document but rare across documents.\n",
        "\n",
        "## TF-IDF\n",
        "\n",
        "### TF-IDF(t,d,D) = TF(t,d) × IDF(t,D)\n",
        "\n",
        "Where:\n",
        "\n",
        "-   t: term\n",
        "\n",
        "-   d: document\n",
        "\n",
        "-   D: corpus \n",
        "\n",
        "-   TF(t,d) = count of t in d / number of words in d\n",
        "\n",
        "-   IDF(t,D) = log(number of d in D / number of documents containing t + 1​)\\\n",
        "\n",
        "## TF-IDF\n",
        "\n",
        "### How to calculate TF-IDF?\n",
        "\n",
        "Using `TfidfVectorizer` from `scikit-learn` library\n"
      ],
      "id": "939f37b7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "column"
      },
      "source": [
        "#| echo: true\n",
        "\n",
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initiate a TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform to get TF-IDF values\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus['text'])\n",
        "\n",
        "# Reorganize the result and display \n",
        "# Extract feature names (terms) \n",
        "feature_names = vectorizer.get_feature_names_out() \n",
        "\n",
        "# Convert sparse matrix to DataFrame \n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names) \n",
        "\n",
        "# Add 'document_id' column from original corpus \n",
        "tfidf_df['document_id'] = corpus['document_id'] \n",
        "\n",
        "# Display the TF-IDF DataFrame \n",
        "print(tfidf_df)"
      ],
      "id": "6bf066ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn\n",
        "\n",
        "Calculate the `TF-IDF` for the text in the `opd_teacher` DataFrame.\n"
      ],
      "id": "4e064ef3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "55819c86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualizing Data\n",
        "\n",
        "## Word Clouds\n",
        "\n",
        "### What is word clouds used for?\n",
        "\n",
        "Explicitly visualize the word and its importance (counts, frequencies, tf-idf scores, etc.) that is indicated by the size of the word.\n",
        "\n",
        "## Word Clouds\n",
        "\n",
        "### How to create word clouds?\n",
        "\n",
        "-   Using `WordCloud` and `generate()` if from the raw text\n",
        "\n",
        "-   Using `WordCloud` and `generate_from_frequencies` if from word counts, word frequencies, tf-idf, and other word importance metrics.\n",
        "\n",
        "## Word Clouds from Word Counts\n"
      ],
      "id": "fe8f7628"
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "column"
      },
      "source": [
        "#| echo: true\n",
        "\n",
        "# Import WordCloud and Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Generate a word cloud with custom coloring function\n",
        "wordcloud = WordCloud(width=800, height=500, background_color='white').generate_from_frequencies(dict(zip(word_counts['word'], word_counts['count'])))\n",
        "\n",
        "# Display the word cloud using imshow()\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Note: plt.imshow() is used primarily to display images, including arrays of data that can represent images such as heatmaps, plots, or in this case, a word cloud generated by the WordCloud library."
      ],
      "id": "8fe335f9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn\n",
        "\n",
        "Create a word cloud based on the word counts for the `opd_teacher` DataFrame.\n"
      ],
      "id": "7c7a60b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "40b4320f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bar Chart\n",
        "\n",
        "### What is bar chart used for?\n",
        "\n",
        "A bar chart is a graphical representation of categorical data using rectangular bars where the lengths or heights of the bars correspond to the values they represent. It is commonly used to compare the values of different categories or to show changes over time for a single category.\n",
        "\n",
        "## Bar Chart based on Word Counts\n",
        "\n",
        "### How to create a bar chart?\n",
        "\n",
        "Using `barh()` from matplotlib\n"
      ],
      "id": "0fbd83ef"
    },
    {
      "cell_type": "code",
      "metadata": {
        "output-location": "column"
      },
      "source": [
        "#| echo: true\n",
        "\n",
        "# Import Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a bar chart using barh()\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(word_counts['word'], word_counts['count'], color='skyblue')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Word')\n",
        "plt.title('The Bar Chart of Word Counts')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to display words in descending order\n",
        "plt.show()"
      ],
      "id": "cceb6277",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn\n",
        "\n",
        "Create a bar chart that filters rows with word counts greater than 500 based on the word counts for the `opd_teacher` DataFrame.\n"
      ],
      "id": "84ae4da3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "8560847d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Small Multiples\n",
        "\n",
        "### What is small multiples used for?\n",
        "\n",
        "\"Small multiples\" refers to a series of small, similar visualizations or charts that are displayed together, typically arranged in a grid. Each visualization in the grid shows a subset of the data or a related aspect of the data, making it easier to compare different categories or trends.\n",
        "\n",
        "## Small Multiples based on Word Frequencies\n",
        "\n",
        "### How to create small multiples?\n",
        "\n",
        "Using `catplot()` from seaborn\n"
      ],
      "id": "12832cf2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "\n",
        "# Import seaborn\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a catplot\n",
        "plot = sns.catplot(data=word_frequencies, kind='bar', x='frequency', y = 'tokens', col='document_id', col_wrap=3)\n",
        "plot.set_titles(col_template=\"{col_name}\", size=10)\n",
        "plt.show()"
      ],
      "id": "41ea0117",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Your Turn\n",
        "\n",
        "Create a small supplies that displays the top 5 words for each resources from the `opd_teacher` DataFrame.\n"
      ],
      "id": "79805a97"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# YOUR CODE IS HERE"
      ],
      "id": "ed534098",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\\n",
        "\\\n",
        "\\\n",
        "\n",
        "\\\n",
        "\\"
      ],
      "id": "b15fe5d3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}